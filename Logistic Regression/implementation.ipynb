{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "I am going to use a different notation from what I am used to as it allows me to express complex ideas for n weights. \\\n",
    "$x = (x_0, x_1, \\ldots, x_n)$\n",
    "where $x$ is the feature vector for n vector and $x_{0}= 1$ \\\n",
    "$\\theta = \\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\theta_{1} \\\\\n",
    "    \\ldots \\\\\n",
    "    \\theta_{n} \\\\ \n",
    "\\end{bmatrix}$ where $\\theta$ is the weight vector\\\n",
    "$h_{\\theta}(x)$ represents the hypothesis or the predicted value given x parameritized by $\\theta$ which is the weight \\\n",
    "$\\theta^{T}(x) = \\theta^{T} \\cdot x = (\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}) \\cdot (x_0, x_1, \\ldots, x_n) = \\sum_{i=0}^{n} \\theta_{i}x_{i} $ represents linear regression model. \\\n",
    "When I do probabilities like $P(y|x;\\theta)$ it just means \"probablity of y given x parameterized\". \\\n",
    "m will denote the size of the dataset. \\\n",
    "$\\vec y$ means the entire set of output vectors \\\n",
    "The superscript $x^{(j)}$ will mean jth example of x.\n",
    "$\\theta$ is the learning rate.\n",
    "\n",
    "### Logistic Regression Intution\n",
    "\n",
    "Logistic Regression is an ML algorithm algorithm deals with binary discrete data (so it's either 0 or 1) since we're dealing with probabilites. \\\n",
    "It relies on the assumption that each outcome independent from the other (important to keep in mind for conditional probability). \\\n",
    "The algorithm uses the sigmoid function to narrow the range of output $h_{\\theta}(x) \\in [0, 1]$.\n",
    "$$g(z) = \\frac{1}{\\exp(-z) + 1}$$\n",
    "$$h_{\\theta}(x) = g(\\theta^{T}(x))$$\n",
    "The probablities for each y either becomes\n",
    "$$P(y = 1|x;\\theta) = h_{\\theta}(x)$$\n",
    "or\n",
    "$$P(y = 0|x;\\theta) = 1 - h_{\\theta}(x)$$\n",
    "Since we're only dealing with $y \\in {0, 1}$, then we can do the following.\n",
    "$$P(y|x;\\theta) = (h_{\\theta}(x))^{y}(1 - h_{\\theta}(x))^{1-y}$$\n",
    "Generalizing it for $\\vec y$ and also using the fact that we're dealing with independent events, we get\n",
    "$$P(\\vec y|x;\\theta) = \\prod_{j=0}^{m}P(y^{(j)}|x^{(j)};\\theta)$$\n",
    "Simplifying the computation (floating error) and also for easier algebric manipulation, we'll use log instead as the likelihood.\n",
    "$$L(\\theta) = \\log{P(\\vec y|x;\\theta)} = \\log{\\left(\\prod_{j=0}^{m}P(y^{(j)}|x^{(j)};\\theta)\\right)} = \\log{\\left(\\prod_{j=0}^{m}(h_{\\theta}(x^{(j)}))^{y^{(j)}}(1 - h_{\\theta}(x^{(j)}))^{1-y^{(j)}}\\right)}$$\n",
    "$$L(\\theta) = \\sum_{j=0}^{m}\\left(y^{(j)}\\log{(h_{\\theta}(x^{(j)}))}+ (1- y^{(j)})\\log{(1 - h_{\\theta}(x^{(j)}))}\\right) $$\n",
    "Instead of dealing with minizing a cost function, we'll be maximizing the likelihood using gradient ascent\n",
    "$$\\theta_{i} := \\theta_{i} + \\alpha\\frac{\\partial}{\\partial \\theta_{i}}L(\\theta)$$\n",
    "Which turns out to be very similar to linear regression\n",
    "$$\\theta_{i} := \\theta_{i} + \\alpha\\sum_{j=0}^{m}(y^{j} - h_{\\theta}(x^{(j)}))x_{i}^{(j)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass       Age  SibSp  Parch      Fare  C  Q  S\n",
      "0         3  34.50000      0      0    7.8292  0  1  0\n",
      "1         3  47.00000      1      0    7.0000  0  0  1\n",
      "2         2  62.00000      0      0    9.6875  0  1  0\n",
      "3         3  27.00000      0      0    8.6625  0  0  1\n",
      "4         3  22.00000      1      1   12.2875  0  0  1\n",
      "..      ...       ...    ...    ...       ... .. .. ..\n",
      "413       3  30.27259      0      0    8.0500  0  0  1\n",
      "414       1  39.00000      0      0  108.9000  1  0  0\n",
      "415       3  38.50000      0      0    7.2500  0  0  1\n",
      "416       3  30.27259      0      0    8.0500  0  0  1\n",
      "417       3  30.27259      1      1   22.3583  1  0  0\n",
      "\n",
      "[418 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_test = X_test.drop(columns=[\"Ticket\", \"Cabin\", \"PassengerId\", \"Name\", \"Sex\"])\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_test = X_test.rename(\n",
    "    columns={\"Embarked_C\": \"C\", \"Embarked_Q\": \"Q\", \"Embarked_S\": \"S\"}\n",
    ")\n",
    "X_test[\"C\"] = X_test[\"C\"].astype(int)\n",
    "X_test[\"Q\"] = X_test[\"Q\"].astype(int)\n",
    "X_test[\"S\"] = X_test[\"S\"].astype(int)\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "print(X_test)\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(\"train.csv\")\n",
    "y = X[\"Survived\"].to_numpy()\n",
    "X = X.drop(columns=[\"Ticket\", \"Cabin\", \"PassengerId\", \"Name\", \"Sex\", \"Survived\"])\n",
    "X = pd.get_dummies(X)\n",
    "X = X.rename(columns={\"Embarked_C\": \"C\", \"Embarked_Q\": \"Q\", \"Embarked_S\": \"S\"})\n",
    "X[\"C\"] = X[\"C\"].astype(int)\n",
    "X[\"Q\"] = X[\"Q\"].astype(int)\n",
    "X[\"S\"] = X[\"S\"].astype(int)\n",
    "X = X.fillna(X.mean())\n",
    "X = X.to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:91]\n",
    "X_test = X[91:-1]\n",
    "y_train = y[:91]\n",
    "y_test = y[91:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wahee\\AppData\\Local\\Temp\\ipykernel_11632\\2473381973.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (np.exp(-z) + 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X):\n",
    "        self.m = len(X)\n",
    "        self.n = X.shape[-1]\n",
    "        self.weights = np.zeros(self.n + 1)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (np.exp(-z) + 1)\n",
    "\n",
    "    def linear_regression(self, x_j):\n",
    "        weights_without_bias = self.weights[:-1]\n",
    "        return np.dot(weights_without_bias, x_j) + self.weights[-1]\n",
    "\n",
    "    def hypothesis(self, x_j):\n",
    "        return self.sigmoid(self.linear_regression(x_j))\n",
    "\n",
    "    def probability_of_y_given_x(self, y_j, x_j):\n",
    "        hypothesis = self.hypothesis(x_j)\n",
    "        return (hypothesis) ** (y_j) * (1 - hypothesis) ** (1 - y_j)\n",
    "\n",
    "    def likelihood(self, X, y):\n",
    "        return np.sum(\n",
    "            [\n",
    "                y[j] * np.log(self.hypothesis(X[j]))\n",
    "                + (1 - y[j]) * np.log(1 - self.hypothesis(X[j]))\n",
    "                for j in range(self.m)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [np.round(model.hypothesis(X[j])).astype(int) for j in range(len(X))]\n",
    "\n",
    "    def gradient_ascent(self, X, y, i):\n",
    "        return L * np.sum(\n",
    "            [(y[j] - self.hypothesis(X[j])) * X[j][i] for j in range(self.m)]\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            self.weights[i] += self.gradient_ascent(X, y, i)\n",
    "        self.weights[-1] += L * np.sum(\n",
    "            [(y[j] - self.hypothesis(X[j])) for j in range(self.m)]\n",
    "        )\n",
    "\n",
    "\n",
    "L = 0.001\n",
    "model = LogisticRegression(X_train)\n",
    "epochs = 1000\n",
    "for _ in range(epochs):\n",
    "    # prediction = model.predict(X_train, y_train)\n",
    "    model.fit(X_train, y_train)\n",
    "y_pred = pd.DataFrame(\n",
    "    model.predict(X_test), columns=[\"Survived\"], index=np.arange(892, 892 + len(X_test))\n",
    ")\n",
    "y_pred.index.name = \"PassengerId\"\n",
    "y_pred.to_csv(\"./submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
