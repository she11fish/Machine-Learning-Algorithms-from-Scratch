{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Support Vector machines is an supervised machine learning algorithm used for binary classification problems. \\\n",
    "It tries to find the hyperplane where it can split the data with the maximum margin possible to predict \\\n",
    "the class of the new data point to be predicted. The equation to express that middle hyperplane is \n",
    "$$\\vec{w} \\cdot \\vec{u} \\geq c \\tag{1}$$\n",
    "where $\\vec{w}$ is a normal vector to the hyperplane, and $\\vec{u}$ is a random vector. \\\n",
    "Equation $(1)$ is if the length of the projected vector $\\vec{u}$ times length of ${w}$ is greater by some constant $c$ \\\n",
    "then it will be in the positive class. If we rewrite the equation to the following\n",
    "$$\\vec{w} \\cdot \\vec{u} + b \\geq 0 \\tag{2}$$\n",
    "where $b := -c$ for convention purposes as b is normal used to indicate the intercept. \\\n",
    "Rewriting it again to the commonly used equation\n",
    "$$\\vec{w} \\cdot \\vec{x_{+}} + b \\geq 1 \\tag{3}$$\n",
    "$$\\vec{w} \\cdot \\vec{x_{-}} + b \\leq -1 \\tag{4}$$\n",
    "where $x_{+}$ is the positive class, and $x_{-}$ is the negative class. \\\n",
    "Define $y_{i}$ to be $y_i = 1$ for positive samples and $y_i = -1$ for negative samples.\n",
    "Combining equations $(3)$ and $(4)$ by multiplying $y_i$, we get\n",
    "$$y_{i}(\\vec{w} \\cdot \\vec{x_{i}} + b) \\geq 1 \\tag{5}$$\n",
    "or\n",
    "$$y_{i}(\\vec{w} \\cdot \\vec{x_{-}} + b) - 1 \\geq 0 \\tag{7}$$\n",
    "If $x_i$ is on the support vector (the line that determines whether a class is positive or negative)\n",
    "$$y_{i}(\\vec{w} \\cdot \\vec{x_{-}} + b) - 1 = 0 \\tag{8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the width of the margin\n",
    "Assume $x_{+}$ and $x_{-}$ denote their respective data class that lie on their support vectors \\\n",
    "The width of the margin relies on the dot product of the normal unit vector of $\\vec{w}$ and vector between the support vectors. \\\n",
    "To find that vector, we can $x_{+} - x_{-}$. We can use the geometric property of dot product and the magnitude of unit vector $(1)$ \\\n",
    "to preserve the distance of $x_{+} - x_{-}$ when doing the dot product, unit vector of $\\vec{w}$ will be used to get the margin. \\\n",
    "This equation will be derived as result\n",
    "$$WIDTH = (x_{+} - x_{-}) \\cdot \\frac{\\vec{w}}{||w||}$$\n",
    "Substuting the definition $x_{+}$ and $x_{-}$ by using equation $(8)$\n",
    "$$WIDTH = (x_{+}\\vec{w} - x_{-}\\vec{w}) \\cdot \\frac{1}{||w||}$$\n",
    "$$WIDTH = (1 - b - (-1 - b)) \\cdot \\frac{1}{||w||}$$\n",
    "$$WIDTH = \\frac{2}{||w||}$$\n",
    "We need to maximize the margin. \n",
    "$$\\max_{||w||} \\frac{2}{||w||}$$\n",
    "$$\\implies \\max_{||w||} \\frac{1}{||w||}$$\n",
    "$$\\implies \\min_{||w||} {||w||}$$\n",
    "For the sake of mathematical convenience\n",
    "$$\\min_{||w||} {\\frac{1}{2}}{||w||^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the max width of the margin\n",
    "We need to find the maximum width of the margin with the constraints included. Now comes the beautiful part that connects everything together. \\\n",
    "Since we have a constraint (equation 8) and have the value to optimize ${\\frac{1}{2}}{||w||^{2}}$, we can use Lagrange multipliers to form \\\n",
    "an equation that takes the thing we want to minimize and the constraint together, giving us. \n",
    "$$\\mathcal{L}(w, b) = \\frac{1}{2}||w||^{2} - \\sum_{i=0}^{n} \\alpha_{i}[y_{i}(\\vec{w} \\cdot \\vec{x_{i}} + b) - 1]$$\n",
    "Taking the derivative of w and b to find the extremum respectively\n",
    "$$\\frac{\\partial{L}}{\\partial{w}} = \\vec{w} - \\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}} = 0 \\implies \\vec{w} = \\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}} $$\n",
    "$$\\frac{\\partial{L}}{\\partial{b}} = - \\sum_{i=0}^{n} \\alpha_{i}y_{i} = 0 \\implies \\sum_{i=0}^{n} \\alpha_{i}y_{i} = 0$$\n",
    "Plugging w into the equation\n",
    "$$\\mathcal{L}(w, b) = \\frac{1}{2}\\left(\\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\right) \\cdot \\left(\\sum_{j=0}^{m} \\alpha_{j}y_{j}\\vec{x_{j}}\\right) - \\left(\\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\right) \\cdot \\left(\\sum_{j=0}^{m} \\alpha_{j}y_{j}\\vec{x_{i}}\\right) - b\\sum_{i=0}^{n} \\alpha_{i}y_{i} + \\sum_{i=0}^{n} \\alpha_{i}$$\n",
    "$$\\mathcal{L}(w, b) = \\frac{1}{2}\\left(\\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\right) \\cdot \\left(\\sum_{j=0}^{m} \\alpha_{j}y_{j}\\vec{x_{j}}\\right) - \\left(\\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\right) \\cdot \\left(\\sum_{j=0}^{m} \\alpha_{j}y_{j}\\vec{x_{i}}\\right) + \\sum_{i=0}^{n} \\alpha_{i}$$\n",
    "$$\\mathcal{L}(w, b) = -\\frac{1}{2}\\left(\\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\right) \\cdot \\left(\\sum_{j=0}^{m} \\alpha_{j}y_{j}\\vec{x_{j}}\\right) + \\sum_{i=0}^{n} \\alpha_{i}$$\n",
    "$$\\mathcal{L}(w, b) = \\sum_{i=0}^{n} \\alpha_{i} -\\frac{1}{2} \\sum_{i=0}^{n} \\sum_{j=0}^{m} \\alpha_{i}\\alpha_{j} y_{i}y_{j} \\vec{x_{i}}\\vec{x_{j}}$$\n",
    "The optimization relies on the dot product of the pairs of samples. \\\n",
    "It even appears in the decision rule.\n",
    "$$ \\sum_{i=0}^{n} \\alpha_{i}y_{i}\\vec{x_{i}}\\vec{u_{i}} + b \\geq 0$$\n",
    "One issue with this is that it doesn't work that are not linearly seperable. \\\n",
    "One way to fix this issue is transforming the points to a more convenient space. \\\n",
    "We denote that transformation as $\\phi(x)$. We need to maximize $\\phi(x_i) \\cdot \\phi(x_j)$ \\\n",
    "The awesome thing is we can define this using kernels to get the follwing\n",
    "$$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$$\n",
    "So, we don't need to know $\\phi(x)$ i.e. the transformation for this to work; we only need to use the kernel to make this work. \\\n",
    "Not only does this help with math, but it also provides less computations and storage needed \\\n",
    "as you don't need to compute the transformation (requires higher dimensions) \\\n",
    "and then the inner product instead of the inner product and kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Lagrange Multipliers\n",
    "There are many ways to find the Lagrange Multipliers. The one I am going to use allows us to find the closed form, meaning we \\\n",
    "can solve the problem using a formula that gets us the Lagrange for fitting the model. \\\n",
    "The algorithm is Sequential Minimal Optimization (SMO). SMO is a type of coordinate ascent algorithm. It's coordinate \\ \n",
    "since we're using $\\alpha_i$ and $\\alpha_j$ iteratively based on some heuristics for determining order to to find $\\alpha$.\n",
    "$$\\max_{\\alpha} \\sum_{i=0}^{n} \\alpha_{i} -\\frac{1}{2} \\sum_{i=0}^{n} \\sum_{j=0}^{m} \\alpha_{i}\\alpha_{j} y_{i}y_{j} \\vec{x_{i}}\\vec{x_{j}} \\text{ } s.t. \\text{ }\n",
    "\\begin{cases} \n",
    "   0 \\leq \\alpha_{i} \\leq C & i = 0, \\ldots, n \\\\\n",
    "   \\sum_{i=0}^{n} \\alpha_{i}y_{i} = 0\n",
    "\\end{cases}$$\n",
    "The issue with why we can't optimze for one alpha is because finding the optimal values does not guarentee that we're within in the constraints. \\\n",
    "Fixing $\\alpha_{1}, \\ldots, \\alpha_{n}$ and trying to optimize $\\alpha_{0}$ we get\n",
    "$$ \\alpha_{0}y_{0} = -\\sum_{i=1}^{n}\\alpha_{i}y_{i}$$\n",
    "$y_{0} = 1 / y_{0}$ so we can multiply $y_{0}$ instead \n",
    "$$ \\alpha_{0} = -y_{0}\\sum_{i=1}^{n}\\alpha_{i}y_{i}$$\n",
    "Let's do this instead. Fix $\\alpha_{2}, \\ldots, \\alpha_{n}$ and trying to optimize $\\alpha_{0}, \\alpha_{1}$ we get\n",
    "$$ \\alpha_{0}y_{0} + \\alpha_{1}y_{1} = -\\sum_{i=2}^{n}\\alpha_{i}y_{i} =: \\zeta$$\n",
    "$$ \\alpha_{0}y_{0}= \\zeta - \\alpha_{1}y_{1}$$\n",
    "$$ \\alpha_{0}= y_{0}(\\zeta - \\alpha_{1}y_{1})$$\n",
    "The idea for the optimization is that if we pick some $\\alpha_{0}$, then $\\alpha_{1} \\in [L, H]$ where $0 \\leq L < H \\leq C$ for \\\n",
    "$\\alpha_{0}y_{0} + \\alpha_{1}y_{1} = \\zeta$ to be satisfied.\n",
    "$$L = max(0, \\alpha_{1} ​− \\alpha_{0}​) \\text{ and } H = min(C, C + \\alpha_{1} ​− \\alpha_{0}​)$$\n",
    "Therefore, our optimization will now be\n",
    "$$\\max_{\\alpha_{1} \\in [L, H]} \\alpha_{0}  + \\alpha_{1} + \\text{ constants } + \\sum_{i=0}^{n} \\sum_{j=0}^{m} \\alpha_{i}\\alpha_{j} y_{i}y_{j} \\vec{x_{i}}\\vec{x_{j}} \\text{ where } \\alpha_{0} = y_{0}(\\zeta -  \\alpha_{1}y_{1})$$\n",
    "Taking the derivative with respect to $\\alpha_{1}$ and setting it to $0$, we get\n",
    "$$\\max_{\\alpha_{1} \\in [L, H]} \\alpha_{0}  + \\alpha_{1} + \\text{ constants } + \\sum_{i=0}^{n} \\sum_{j=0}^{m} \\alpha_{i}\\alpha_{j} y_{i}y_{j} \\vec{x_{i}}\\vec{x_{j}} \\text{ where } \\alpha_{0} = y_{0}(\\zeta -  \\alpha_{1}y_{1})$$\n",
    "$$\\max_{\\alpha_{1} \\in [L, H]} y_{0}(\\zeta -  \\alpha_{1}y_{1})  + \\alpha_{1} + \\sum_{i=0}^{n} \\sum_{j=0}^{m} \\alpha_{i}\\alpha_{j} y_{i}y_{j} \\vec{x_{i}}\\vec{x_{j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_1 = \\frac{-y_0y_1 + 1 + 2 \\sum_{i \\neq 1} (\\alpha_i y_i y_1 \\mathbf{x}_i \\cdot \\mathbf{x}_1)}{-2 ||\\mathbf{x}_1||^2}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
